{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bot_functions as bf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport bot_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of ticker symbols\n",
    "sp500_tickers = pd.read_csv('sp500.csv').set_index('Unnamed: 0')['Symbol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get keys from json file. Returns dictionary to pass into start_bot()\n",
    "# You can also make a dictionary with keys \"user\" and \"pass\"\n",
    "my_keys = bf.get_keys(\"/Users/indez/.secret/tda/tda_keys.json\")\n",
    "# or\n",
    "# my_keys = {'user':'your_td_username', 'pass':'your_td_password'}\n",
    "\n",
    "# Get API key:\n",
    "keys = bf.get_keys(\"/Users/indez/.secret/tda/tda.json\")\n",
    "api_key = keys[\"consumer_key\"]\n",
    "# Set directory to save/load access token to\n",
    "token_path = \"/Users/indez/.secret/tda/toke\"\n",
    "# Redirect URL of your app, do not change unless you have a remotely hosted app\n",
    "redirect_url = \"https://localhost\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a webdriver and start up the bot\n",
    "driver = bf.start_bot(my_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next three cells all run the same code, but they show an actual workflow day of mine\n",
    "# where errors occurred because my screen went dark during the process. Remember to turn off\n",
    "# your power saving settings, as well as adjust your TD Ameritrade account settings to not log\n",
    "# you off while this is running.\n",
    "# I left all the cells in to show how the scraper can be run in chunks without having to worry\n",
    "# about duplicate files, and that if errors are encountered one can just restart the kernel and\n",
    "# then pick up where they left off. \n",
    "\n",
    "# I highly recommend preserving your output cells so that you can look back and see what might\n",
    "# need fixing, or any other details that may come in handly later when you are trying to use\n",
    "# the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to gather summary for FLT on attempt 1\n",
      "Failed to gather summary for FLT on attempt 2\n",
      "Failed to gather summary for FLT on attempt 3\n",
      "Failed to gather summary for FLT on attempt 4\n",
      "Failed to gather summary for FLT on attempt 5\n",
      "Too many failed attempts for summary of FLT, skipping to next df.\n",
      "Failed to gather earnings for FLT on attempt 1\n",
      "Failed to gather earnings for FLT on attempt 2\n",
      "Failed to gather earnings for FLT on attempt 3\n",
      "Failed to gather earnings for FLT on attempt 4\n",
      "Failed to gather earnings for FLT on attempt 5\n",
      "Too many failed attempts for earnings of FLT, skipping to next df.\n",
      "Failed to gather fundamentals for FLT on attempt 1\n",
      "Failed to gather fundamentals for FLT on attempt 2\n",
      "Failed to gather fundamentals for FLT on attempt 3\n",
      "Failed to gather fundamentals for FLT on attempt 4\n",
      "Failed to gather fundamentals for FLT on attempt 5\n",
      "Too many failed attempts for fundamentals of FLT, skipping to next df.\n",
      "Failed to gather valuation for FLT on attempt 1\n",
      "Failed to gather valuation for FLT on attempt 2\n",
      "Failed to gather valuation for FLT on attempt 3\n",
      "Failed to gather valuation for FLT on attempt 4\n",
      "Failed to gather valuation for FLT on attempt 5\n",
      "Too many failed attempts for valuation of FLT, skipping to next df.\n",
      "Failed to gather analysts for FLT on attempt 1\n",
      "Failed to gather analysts for FLT on attempt 2\n",
      "Failed to gather analysts for FLT on attempt 3\n",
      "Failed to gather analysts for FLT on attempt 4\n",
      "Failed to gather analysts for FLT on attempt 5\n",
      "Too many failed attempts for analysts of FLT, skipping to next df.\n",
      "Did not successfully scrape FLT\n"
     ]
    }
   ],
   "source": [
    "flt = bf.scrape_watchlist(driver, ['FLT'], 'sp500_fix', root_dir='D:/Databases/TDA/',\n",
    "                          skip_finished=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: window was already closed\n  (Session info: chrome=84.0.4147.135)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-7f4b2560b19e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msearch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"search\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'FLT'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\switch_to.py\u001b[0m in \u001b[0;36mdefault_content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \"\"\"\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_driver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSWITCH_TO_FRAME\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe_reference\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNoSuchWindowException\u001b[0m: Message: no such window: window was already closed\n  (Session info: chrome=84.0.4147.135)\n"
     ]
    }
   ],
   "source": [
    "driver.switch_to.default_content()\n",
    "search = driver.find_element_by_name(\"search\")\n",
    "search.send_keys('FLT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to gather summary for MMC on attempt 1\n",
      "Failed to gather summary for MMC on attempt 2\n",
      "Failed to gather summary for MMC on attempt 3\n",
      "Failed to gather summary for MMC on attempt 4\n",
      "Failed to gather summary for MMC on attempt 5\n",
      "Too many failed attempts for summary of MMC, skipping to next df.\n",
      "Failed to gather earnings for MMC on attempt 1\n",
      "Failed to gather earnings for MMC on attempt 2\n",
      "Failed to gather earnings for MMC on attempt 3\n",
      "Failed to gather earnings for MMC on attempt 4\n",
      "Failed to gather earnings for MMC on attempt 5\n",
      "Too many failed attempts for earnings of MMC, skipping to next df.\n",
      "Failed to gather fundamentals for MMC on attempt 1\n",
      "Failed to gather fundamentals for MMC on attempt 2\n",
      "Failed to gather fundamentals for MMC on attempt 3\n",
      "Failed to gather fundamentals for MMC on attempt 4\n",
      "Failed to gather fundamentals for MMC on attempt 5\n",
      "Too many failed attempts for fundamentals of MMC, skipping to next df.\n",
      "Failed to gather valuation for MMC on attempt 1\n",
      "Failed to gather valuation for MMC on attempt 2\n",
      "Failed to gather valuation for MMC on attempt 3\n",
      "Failed to gather valuation for MMC on attempt 4\n",
      "Failed to gather valuation for MMC on attempt 5\n",
      "Too many failed attempts for valuation of MMC, skipping to next df.\n",
      "Failed to gather analysts for MMC on attempt 1\n",
      "Failed to gather analysts for MMC on attempt 2\n",
      "Failed to gather analysts for MMC on attempt 3\n",
      "Failed to gather analysts for MMC on attempt 4\n",
      "Failed to gather analysts for MMC on attempt 5\n",
      "Too many failed attempts for analysts of MMC, skipping to next df.\n",
      "Did not successfully scrape MMC\n",
      "Failed to gather summary for TROW on attempt 1\n",
      "Failed to gather summary for TROW on attempt 2\n",
      "Failed to gather summary for TROW on attempt 3\n",
      "Failed to gather summary for TROW on attempt 4\n",
      "Failed to gather summary for TROW on attempt 5\n",
      "Too many failed attempts for summary of TROW, skipping to next df.\n",
      "Failed to gather earnings for TROW on attempt 1\n",
      "Failed to gather earnings for TROW on attempt 2\n",
      "Failed to gather earnings for TROW on attempt 3\n",
      "Failed to gather earnings for TROW on attempt 4\n",
      "Failed to gather earnings for TROW on attempt 5\n",
      "Too many failed attempts for earnings of TROW, skipping to next df.\n",
      "Failed to gather fundamentals for TROW on attempt 1\n",
      "Failed to gather fundamentals for TROW on attempt 2\n",
      "Failed to gather fundamentals for TROW on attempt 3\n",
      "Failed to gather fundamentals for TROW on attempt 4\n",
      "Failed to gather fundamentals for TROW on attempt 5\n",
      "Too many failed attempts for fundamentals of TROW, skipping to next df.\n",
      "Failed to gather valuation for TROW on attempt 1\n",
      "Failed to gather valuation for TROW on attempt 2\n",
      "Failed to gather valuation for TROW on attempt 3\n",
      "Failed to gather valuation for TROW on attempt 4\n",
      "Failed to gather valuation for TROW on attempt 5\n",
      "Too many failed attempts for valuation of TROW, skipping to next df.\n",
      "Failed to gather analysts for TROW on attempt 1\n",
      "Failed to gather analysts for TROW on attempt 2\n",
      "Failed to gather analysts for TROW on attempt 3\n",
      "Failed to gather analysts for TROW on attempt 4\n",
      "Failed to gather analysts for TROW on attempt 5\n",
      "Too many failed attempts for analysts of TROW, skipping to next df.\n",
      "Did not successfully scrape TROW\n"
     ]
    }
   ],
   "source": [
    "test = bf.scrape_watchlist(driver, ['MMC', 'TROW'], 'pairs', root_dir='D:/Databases/TDA/',\n",
    "                          skip_finished=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to gather fundamentals for ADBE on attempt 1\n",
      "Failed to gather fundamentals for ADBE on attempt 2\n",
      "Failed to gather fundamentals for ADBE on attempt 3\n",
      "Failed to gather fundamentals for ADBE on attempt 4\n",
      "Failed to gather fundamentals for ADBE on attempt 5\n",
      "Too many failed attempts for fundamentals of ADBE, skipping to next df.\n",
      "Failed to gather valuation for ADBE on attempt 1\n",
      "Failed to gather valuation for ADBE on attempt 2\n",
      "Failed to gather valuation for ADBE on attempt 3\n",
      "Failed to gather valuation for ADBE on attempt 4\n",
      "Failed to gather valuation for ADBE on attempt 5\n",
      "Too many failed attempts for valuation of ADBE, skipping to next df.\n",
      "Failed to gather analysts for ADBE on attempt 1\n",
      "Failed to gather analysts for ADBE on attempt 2\n",
      "Failed to gather analysts for ADBE on attempt 3\n",
      "Failed to gather analysts for ADBE on attempt 4\n",
      "Failed to gather analysts for ADBE on attempt 5\n",
      "Too many failed attempts for analysts of ADBE, skipping to next df.\n",
      "10 tickers scraped\n",
      "Failed to gather summary for ADM on attempt 1\n",
      "Failed to gather summary for ADM on attempt 2\n",
      "Failed to gather fundamentals for ADS on attempt 1\n",
      "Failed to gather fundamentals for AEE on attempt 1\n",
      "Failed to gather fundamentals for AEP on attempt 1\n",
      "Failed to gather fundamentals for AEP on attempt 2\n",
      "Failed to gather summary for AGN on attempt 1\n",
      "Failed to gather summary for AGN on attempt 2\n",
      "Failed to gather summary for AGN on attempt 3\n",
      "Failed to gather summary for AGN on attempt 4\n",
      "Failed to gather summary for AGN on attempt 5\n",
      "Too many failed attempts for summary of AGN, skipping to next df.\n",
      "Failed to gather earnings for AGN on attempt 1\n",
      "Failed to gather earnings for AGN on attempt 2\n",
      "Failed to gather earnings for AGN on attempt 3\n",
      "Failed to gather earnings for AGN on attempt 4\n",
      "Failed to gather earnings for AGN on attempt 5\n",
      "Too many failed attempts for earnings of AGN, skipping to next df.\n",
      "Failed to gather fundamentals for AGN on attempt 1\n",
      "Failed to gather fundamentals for AGN on attempt 2\n",
      "Failed to gather fundamentals for AGN on attempt 3\n",
      "Failed to gather fundamentals for AGN on attempt 4\n",
      "Failed to gather fundamentals for AGN on attempt 5\n",
      "Too many failed attempts for fundamentals of AGN, skipping to next df.\n",
      "Failed to gather valuation for AGN on attempt 1\n",
      "Failed to gather valuation for AGN on attempt 2\n",
      "Failed to gather valuation for AGN on attempt 3\n",
      "Failed to gather valuation for AGN on attempt 4\n",
      "Failed to gather valuation for AGN on attempt 5\n",
      "Too many failed attempts for valuation of AGN, skipping to next df.\n",
      "Failed to gather analysts for AGN on attempt 1\n",
      "Failed to gather analysts for AGN on attempt 2\n",
      "Failed to gather analysts for AGN on attempt 3\n",
      "Failed to gather analysts for AGN on attempt 4\n",
      "Failed to gather analysts for AGN on attempt 5\n",
      "Too many failed attempts for analysts of AGN, skipping to next df.\n",
      "Did not successfully scrape AGN\n",
      "Failed to gather summary for AIG on attempt 1\n",
      "Failed to gather earnings for AIG on attempt 1\n",
      "Failed to gather earnings for AIG on attempt 2\n",
      "Failed to gather earnings for AIG on attempt 3\n",
      "Failed to gather earnings for AIG on attempt 4\n",
      "Failed to gather earnings for AIG on attempt 5\n",
      "Too many failed attempts for earnings of AIG, skipping to next df.\n",
      "Failed to gather fundamentals for AIG on attempt 1\n",
      "Failed to gather fundamentals for AIG on attempt 2\n",
      "Failed to gather fundamentals for AIG on attempt 3\n",
      "Failed to gather fundamentals for AIG on attempt 4\n",
      "Failed to gather fundamentals for AIG on attempt 5\n",
      "Too many failed attempts for fundamentals of AIG, skipping to next df.\n"
     ]
    }
   ],
   "source": [
    "# First attempt at a big scrape. At FIS, the computer screen went black, so in the next cell,\n",
    "# run the same code and it will skip all the tickers that were already done.\n",
    "big_df, skipped = bf.scrape_watchlist(driver, \n",
    "                                      sp500_tickers, \n",
    "                                      'sp500_close', \n",
    "                                      root_dir='D:/Databases/TDA/', # If none passed will use current directory\n",
    "                                      skip_finished=True,\n",
    "                                      errors='ignore',\n",
    "                                      return_skipped=True,\n",
    "                                      internet_speed='slow'\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to gather fundamentals for EW on attempt 1\n",
      "Failed to gather summary for EXC on attempt 1\n",
      "170 tickers scraped\n",
      "180 tickers scraped\n",
      "Failed to gather fundamentals for FMC on attempt 1\n",
      "Failed to gather fundamentals for FMC on attempt 2\n",
      "Failed to gather fundamentals for FMC on attempt 3\n",
      "Failed to gather fundamentals for FMC on attempt 4\n",
      "Failed to gather fundamentals for FMC on attempt 5\n",
      "Too many failed attempts for fundamentals of FMC, skipping to next df.\n",
      "Failed to gather valuation for FMC on attempt 1\n",
      "Failed to gather summary for FOX on attempt 1\n",
      "Failed to gather earnings for FOX on attempt 1\n",
      "Failed to gather earnings for FOX on attempt 2\n",
      "Failed to gather earnings for FOX on attempt 3\n",
      "Failed to gather earnings for FOX on attempt 4\n",
      "Failed to gather earnings for FOX on attempt 5\n",
      "Too many failed attempts for earnings of FOX, skipping to next df.\n",
      "Historic Growth not available for FOX\n",
      "Short Interest info not available for FOX\n",
      "190 tickers scraped\n",
      "Historic Growth not available for FOXA\n",
      "Historic Growth not available for FTI\n",
      "200 tickers scraped\n",
      "Short Interest info not available for GOOG\n",
      "Failed to gather fundamentals for GOOGL on attempt 1\n",
      "Failed to gather fundamentals for GOOGL on attempt 2\n",
      "Historic Growth not available for GPN\n",
      "210 tickers scraped\n",
      "Failed to gather summary for GWW on attempt 1\n",
      "Failed to gather fundamentals for GWW on attempt 1\n",
      "220 tickers scraped\n",
      "230 tickers scraped\n",
      "Failed to gather fundamentals for HSY on attempt 1\n",
      "240 tickers scraped\n",
      "Failed to gather fundamentals for IPG on attempt 1\n",
      "250 tickers scraped\n",
      "260 tickers scraped\n",
      "Failed to gather analysts for JWN on attempt 1\n",
      "Failed to gather analysts for JWN on attempt 2\n",
      "Failed to gather analysts for JWN on attempt 3\n",
      "Failed to gather analysts for JWN on attempt 4\n",
      "Failed to gather analysts for JWN on attempt 5\n",
      "Too many failed attempts for analysts of JWN, skipping to next df.\n"
     ]
    }
   ],
   "source": [
    "# Second scrape, starts where the other left off based on the completed files in the directory\n",
    "# At MET, TD Ameritrade logged me out, so a third scrape was needed to finish the SP500\n",
    "big_df, skipped = bf.scrape_watchlist(driver, \n",
    "                                      sp500_tickers, \n",
    "                                      'sp500_close', \n",
    "                                      root_dir='D:/Databases/TDA/', # If none passed will use current directory\n",
    "                                      skip_finished=True,\n",
    "                                      errors='ignore',\n",
    "                                      return_skipped=True,\n",
    "                                      internet_speed='slow'\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to gather summary for KIM on attempt 1\n",
      "270 tickers scraped\n",
      "Failed to gather earnings for L on attempt 1\n",
      "Failed to gather earnings for L on attempt 2\n",
      "Failed to gather earnings for L on attempt 3\n",
      "Failed to gather earnings for L on attempt 4\n",
      "Failed to gather earnings for L on attempt 5\n",
      "Too many failed attempts for earnings of L, skipping to next df.\n",
      "280 tickers scraped\n",
      "Historic Growth not available for LHX\n",
      "290 tickers scraped\n",
      "Failed to gather summary for LW on attempt 1\n",
      "Failed to gather fundamentals for LW on attempt 1\n",
      "Failed to gather fundamentals for M on attempt 1\n",
      "300 tickers scraped\n",
      "Failed to gather summary for MCO on attempt 1\n",
      "310 tickers scraped\n",
      "Failed to gather fundamentals for MRK on attempt 1\n",
      "Failed to gather fundamentals for MRK on attempt 2\n",
      "320 tickers scraped\n",
      "Failed to gather fundamentals for MS on attempt 1\n",
      "Failed to gather fundamentals for MXIM on attempt 1\n",
      "Failed to gather summary for NBL on attempt 1\n",
      "330 tickers scraped\n",
      "Failed to gather fundamentals for NDAQ on attempt 1\n",
      "Failed to gather fundamentals for NFLX on attempt 1\n",
      "Failed to gather fundamentals for NFLX on attempt 2\n",
      "Failed to gather summary for NI on attempt 1\n",
      "340 tickers scraped\n",
      "Failed to gather summary for NOV on attempt 1\n",
      "Failed to gather summary for NOV on attempt 2\n",
      "Failed to gather summary for NOV on attempt 3\n",
      "Failed to gather summary for NOV on attempt 4\n",
      "Too many failed attempts for summary of NOV, skipping to next df.\n",
      "Failed to gather earnings for NOV on attempt 1\n",
      "Failed to gather earnings for NOV on attempt 2\n",
      "Failed to gather earnings for NOV on attempt 3\n",
      "Failed to gather earnings for NOV on attempt 4\n",
      "Too many failed attempts for earnings of NOV, skipping to next df.\n",
      "Failed to gather fundamentals for NOV on attempt 1\n",
      "Did not successfully scrape NOV\n",
      "Failed to gather fundamentals for NOW on attempt 1\n",
      "Failed to gather fundamentals for NRG on attempt 1\n",
      "Failed to gather fundamentals for NRG on attempt 2\n",
      "Failed to gather fundamentals for NRG on attempt 3\n",
      "Failed to gather fundamentals for NVDA on attempt 1\n",
      "Failed to gather fundamentals for NVDA on attempt 2\n",
      "Failed to gather analysts for NWL on attempt 1\n",
      "Failed to gather analysts for NWL on attempt 2\n",
      "Failed to gather analysts for NWL on attempt 3\n",
      "Failed to gather analysts for NWL on attempt 4\n",
      "Failed to gather analysts for NWL on attempt 5\n",
      "Too many failed attempts for analysts of NWL, skipping to next df.\n",
      "350 tickers scraped\n",
      "Failed to gather summary for NWS on attempt 1\n",
      "Failed to gather summary for NWS on attempt 2\n",
      "Failed to gather summary for NWS on attempt 3\n",
      "Failed to gather summary for NWS on attempt 4\n"
     ]
    }
   ],
   "source": [
    "# Second scrape, starts where the other left off based on the completed files in the directory\n",
    "# At MET, TD Ameritrade logged me out, so a third scrape was needed to finish the SP500\n",
    "big_df, skipped = bf.scrape_watchlist(driver, \n",
    "                                      sp500_tickers, \n",
    "                                      'sp500_close', \n",
    "                                      root_dir='D:/Databases/TDA/', # If none passed will use current directory\n",
    "                                      skip_finished=True,\n",
    "                                      errors='ignore',\n",
    "                                      return_skipped=True,\n",
    "                                      internet_speed='slow'\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to gather fundamentals for NOV on attempt 1\n",
      "Failed to gather earnings for NWS on attempt 1\n",
      "Failed to gather earnings for NWS on attempt 2\n",
      "Failed to gather earnings for NWS on attempt 3\n",
      "Failed to gather earnings for NWS on attempt 4\n",
      "Failed to gather earnings for NWS on attempt 5\n",
      "Too many failed attempts for earnings of NWS, skipping to next df.\n",
      "Short Interest info not available for NWS\n",
      "Failed to gather fundamentals for ODFL on attempt 1\n",
      "Failed to gather fundamentals for ODFL on attempt 2\n",
      "360 tickers scraped\n",
      "Failed to gather summary for PEP on attempt 1\n",
      "370 tickers scraped\n",
      "Failed to gather summary for PNC on attempt 1\n",
      "380 tickers scraped\n",
      "Historic Growth not available for PRGO\n",
      "390 tickers scraped\n",
      "400 tickers scraped\n",
      "410 tickers scraped\n",
      "Failed to gather fundamentals for SLB on attempt 1\n",
      "420 tickers scraped\n",
      "Failed to gather summary for STX on attempt 1\n",
      "Failed to gather fundamentals for SWKS on attempt 1\n",
      "430 tickers scraped\n",
      "Failed to gather fundamentals for TFC on attempt 1\n",
      "Failed to gather fundamentals for TFC on attempt 2\n",
      "Failed to gather fundamentals for TFC on attempt 3\n",
      "Failed to gather fundamentals for TFC on attempt 4\n",
      "Failed to gather fundamentals for TFC on attempt 5\n",
      "Too many failed attempts for fundamentals of TFC, skipping to next df.\n",
      "Failed to gather valuation for TFC on attempt 1\n",
      "Failed to gather valuation for TFC on attempt 2\n",
      "Failed to gather valuation for TFC on attempt 3\n",
      "Failed to gather valuation for TFC on attempt 4\n",
      "Failed to gather valuation for TFC on attempt 5\n",
      "Too many failed attempts for valuation of TFC, skipping to next df.\n",
      "Failed to gather analysts for TFC on attempt 1\n",
      "Failed to gather analysts for TFC on attempt 2\n",
      "Failed to gather analysts for TFC on attempt 3\n",
      "Failed to gather analysts for TFC on attempt 4\n",
      "Failed to gather analysts for TFC on attempt 5\n",
      "Too many failed attempts for analysts of TFC, skipping to next df.\n",
      "440 tickers scraped\n",
      "450 tickers scraped\n",
      "Short Interest info not available for UA\n",
      "460 tickers scraped\n",
      "Historic Growth not available for VFC\n",
      "470 tickers scraped\n",
      "480 tickers scraped\n",
      "490 tickers scraped\n",
      "500 tickers scraped\n"
     ]
    }
   ],
   "source": [
    "# Second scrape, starts where the other left off based on the completed files in the directory\n",
    "# At MET, TD Ameritrade logged me out, so a third scrape was needed to finish the SP500\n",
    "big_df, skipped = bf.scrape_watchlist(driver, \n",
    "                                      sp500_tickers, \n",
    "                                      'sp500_close', \n",
    "                                      root_dir='D:/Databases/TDA/', # If none passed will use current directory\n",
    "                                      skip_finished=True,\n",
    "                                      errors='ignore',\n",
    "                                      return_skipped=True,\n",
    "                                      internet_speed='slow'\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 tickers scraped\n"
     ]
    }
   ],
   "source": [
    "# Second scrape, starts where the other left off based on the completed files in the directory\n",
    "# At MET, TD Ameritrade logged me out, so a third scrape was needed to finish the SP500\n",
    "big_df, skipped = bf.scrape_watchlist(driver, \n",
    "                                      sp500_tickers, \n",
    "                                      'sp500_close', \n",
    "                                      root_dir='D:/Databases/TDA/', # If none passed will use current directory\n",
    "                                      skip_finished=True,\n",
    "                                      errors='ignore',\n",
    "                                      return_skipped=True,\n",
    "                                      internet_speed='slow'\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310 tickers scraped\n",
      "320 tickers scraped\n",
      "330 tickers scraped\n",
      "340 tickers scraped\n",
      "350 tickers scraped\n",
      "Failed to gather earnings for NWS on attempt 1\n",
      "Failed to gather earnings for NWS on attempt 2\n",
      "Failed to gather earnings for NWS on attempt 3\n",
      "Failed to gather earnings for NWS on attempt 4\n",
      "Failed to gather earnings for NWS on attempt 5\n",
      "Too many failed attempts for earnings of NWS, skipping to next df.\n",
      "Short Interest info not available for NWS\n",
      "360 tickers scraped\n",
      "370 tickers scraped\n",
      "380 tickers scraped\n",
      "Historic Growth not available for PRGO\n",
      "390 tickers scraped\n",
      "400 tickers scraped\n",
      "410 tickers scraped\n",
      "420 tickers scraped\n",
      "430 tickers scraped\n",
      "440 tickers scraped\n",
      "450 tickers scraped\n",
      "Short Interest info not available for UA\n",
      "460 tickers scraped\n",
      "Historic Growth not available for VFC\n",
      "470 tickers scraped\n",
      "480 tickers scraped\n",
      "490 tickers scraped\n",
      "500 tickers scraped\n"
     ]
    }
   ],
   "source": [
    "# Third and final scrape\n",
    "fix_df, skipped = bf.scrape_watchlist(driver, \n",
    "                                      sp500_tickers, \n",
    "                                      'sp500_close', \n",
    "                                      skip_finished=True,\n",
    "                                      save_df=False,\n",
    "                                      errors='ignore',\n",
    "                                      return_skipped=True\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That did it. Now quit your webdriver and start the data_exploration.ipynb to investigate\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
